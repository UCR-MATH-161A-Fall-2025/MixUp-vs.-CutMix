{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "raFweHtx8RcE"
      },
      "outputs": [],
      "source": [
        "#MATH161: CIFAR-10 MixUp vs CutMix with AdamW\n",
        "\n",
        "# Setup — configs, utils, model, MixUp/CutMix\n",
        "import os, csv, random, numpy as np, torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "# configs\n",
        "def _merge(a, b):\n",
        "    out = dict(a)\n",
        "    for k, v in b.items():\n",
        "        if isinstance(v, dict) and isinstance(out.get(k), dict):\n",
        "            out[k] = _merge(out[k], v)\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "BASE = {\n",
        "    \"seed\": 42,\n",
        "    \"epochs\": 100,\n",
        "    \"batch_size\": 128,\n",
        "    \"num_workers\": 2,  # set 2 for Colab/low-core systems to avoid worker warning\n",
        "    \"amp\": True,\n",
        "    \"label_smoothing\": 0.0,\n",
        "    \"optimizer\": {\"name\": \"adamw\", \"lr\": 5e-4, \"weight_decay\": 0.02, \"betas\": (0.9, 0.999), \"eps\": 1e-8},\n",
        "    \"scheduler\": {\"warmup_epochs\": 5},\n",
        "    \"model\": {\"name\": \"resnet18_cifar\"},\n",
        "    \"data\": {\n",
        "        \"root\": \"./data\",\n",
        "        \"mean\": (0.4914, 0.4822, 0.4465),\n",
        "        \"std\":  (0.2470, 0.2435, 0.2616),\n",
        "    },\n",
        "    \"augment_mode\": \"off\",   # off | mixup | cutmix\n",
        "    \"mixup_alpha\": 0.0,\n",
        "    \"cutmix_alpha\": 0.0,\n",
        "    \"save_dir\": \"results/run\",\n",
        "}\n",
        "MIXUP  = {\"augment_mode\": \"mixup\",  \"mixup_alpha\": 0.4, \"save_dir\": \"results/mixup\"}\n",
        "CUTMIX = {\"augment_mode\": \"cutmix\", \"cutmix_alpha\": 1.0, \"save_dir\": \"results/cutmix\"}\n",
        "\n",
        "PROFILES = {\"base\": BASE, \"mixup\": _merge(BASE, MIXUP), \"cutmix\": _merge(BASE, CUTMIX)}\n",
        "\n",
        "def get_config(profile: str, overrides: dict | None = None):\n",
        "    if profile not in PROFILES:\n",
        "        raise ValueError(f\"unknown profile: {profile}\")\n",
        "    cfg = dict(PROFILES[profile])\n",
        "    if overrides:\n",
        "        cfg = _merge(cfg, overrides)\n",
        "    return cfg\n",
        "\n",
        "#  io + reproducibility\n",
        "def ensure_dir(d): os.makedirs(d, exist_ok=True)\n",
        "def save_ckpt(state, path): torch.save(state, path)\n",
        "def csv_logger(path):\n",
        "    f = open(path, \"w\", newline=\"\")\n",
        "    w = csv.DictWriter(f, fieldnames=[\"epoch\",\"train_loss\",\"val_loss\",\"val_acc\"])\n",
        "    w.writeheader()\n",
        "    return f, w\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "#  optimizer and scheduler\n",
        "def build_optimizer(model, cfg):\n",
        "    opt = cfg[\"optimizer\"]\n",
        "    if opt[\"name\"].lower() != \"adamw\":\n",
        "        raise ValueError(\"This setup uses AdamW for both MixUp and CutMix.\")\n",
        "    return AdamW(model.parameters(), lr=opt[\"lr\"], weight_decay=opt[\"weight_decay\"],\n",
        "                 betas=opt.get(\"betas\", (0.9,0.999)), eps=opt.get(\"eps\", 1e-8))\n",
        "\n",
        "def build_warmup_cosine(optimizer, steps_per_epoch: int, epochs: int, warmup_epochs: int):\n",
        "    warmup_iters = max(1, warmup_epochs * steps_per_epoch)\n",
        "    total_iters  = max(1, epochs * steps_per_epoch)\n",
        "    cosine_iters = max(1, total_iters - warmup_iters)\n",
        "    warmup = LinearLR(optimizer, start_factor=1e-3, end_factor=1.0, total_iters=warmup_iters)\n",
        "    cosine = CosineAnnealingLR(optimizer, T_max=cosine_iters)\n",
        "    return SequentialLR(optimizer, [warmup, cosine], milestones=[warmup_iters])\n",
        "\n",
        "#  model\n",
        "def build_model(name: str, num_classes=10):\n",
        "    if name != \"resnet18_cifar\":\n",
        "        raise ValueError(f\"unsupported model: {name}\")\n",
        "    m = resnet18(weights=None, num_classes=num_classes)\n",
        "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    m.maxpool = nn.Identity()\n",
        "    return m\n",
        "\n",
        "#  MixUp / CutMix\n",
        "def mixup_data(x, y, alpha: float):\n",
        "    lam = torch.distributions.Beta(alpha, alpha).sample().item() if alpha and alpha > 0 else 1.0\n",
        "    idx = torch.randperm(x.size(0), device=x.device)\n",
        "    return lam * x + (1 - lam) * x[idx], y, y[idx], lam\n",
        "\n",
        "def mixup_criterion(crit, pred, y_a, y_b, lam):\n",
        "    return lam * crit(pred, y_a) + (1 - lam) * crit(pred, y_b)\n",
        "\n",
        "def _rand_bbox(W, H, lam):\n",
        "    r = (1 - lam) ** 0.5\n",
        "    cw, ch = int(W*r), int(H*r)\n",
        "    cx, cy = np.random.randint(W), np.random.randint(H)\n",
        "    x1, y1 = np.clip(cx - cw//2, 0, W), np.clip(cy - ch//2, 0, H)\n",
        "    x2, y2 = np.clip(cx + cw//2, 0, W), np.clip(cy + ch//2, 0, H)\n",
        "    return x1, y1, x2, y2\n",
        "\n",
        "def cutmix_data(x, y, alpha: float):\n",
        "    lam = torch.distributions.Beta(alpha, alpha).sample().item() if alpha and alpha > 0 else 1.0\n",
        "    bs, _, H, W = x.size()\n",
        "    idx = torch.randperm(bs, device=x.device)\n",
        "    x1, y1, x2, y2 = _rand_bbox(W, H, lam)\n",
        "    x[:, :, y1:y2, x1:x2] = x[idx, :, y1:y2, x1:x2]\n",
        "    lam = 1 - ((x2 - x1) * (y2 - y1)) / (W * H)  # exact area ratio\n",
        "    return x, y, y[idx], lam\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR10 MixUp vs CutMix + AdamW\n",
        "# Wire everything together: data, model, optimizer/scheduler, augmentation, AMP, training loop, validation, and logging.\n",
        "# Per-iteration stepping keeps LR in sync with updates; AMP speeds up on modern GPUs with low complexity.\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "from contextlib import nullcontext  # for AMP shim\n",
        "\n",
        "# ---------------- AMP shim (portable across torch versions / CPU-GPU) ----------------\n",
        "def make_scaler(enabled: bool):\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.amp.GradScaler(enabled=enabled)\n",
        "    # CPU: no-op scaler with same API\n",
        "    class _Dummy:\n",
        "        def scale(self, x): return x\n",
        "        def step(self, opt): opt.step()\n",
        "        def update(self): pass\n",
        "    return _Dummy()\n",
        "\n",
        "def amp_autocast(enabled: bool):\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.amp.autocast(enabled=enabled)\n",
        "    return nullcontext()\n",
        "\n",
        "#  args\n",
        "def parse_args(argv=None):\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--profile\", choices=[\"base\",\"mixup\",\"cutmix\"],\n",
        "                    default=os.environ.get(\"PROFILE\",\"mixup\"),\n",
        "                    help=\"experiment profile; default='mixup' (override with PROFILE env var)\")\n",
        "    ap.add_argument(\"--save_dir\", type=str, default=None)\n",
        "    ap.add_argument(\"--epochs\", type=int, default=None)\n",
        "    ap.add_argument(\"--batch_size\", type=int, default=None)\n",
        "    ap.add_argument(\"--lr\", type=float, default=None)\n",
        "    ap.add_argument(\"--weight_decay\", type=float, default=None)\n",
        "    ap.add_argument(\"--mixup_alpha\", type=float, default=None)\n",
        "    ap.add_argument(\"--cutmix_alpha\", type=float, default=None)\n",
        "    if argv is None:  # ignore IPython/Colab argv in notebooks\n",
        "        argv = []\n",
        "    return ap.parse_args(argv)\n",
        "\n",
        "#  transforms\n",
        "def build_transforms(mean, std):\n",
        "    train_tfms = T.Compose([\n",
        "        T.RandomCrop(32, padding=4),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std)\n",
        "    ])\n",
        "    test_tfms = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean, std)\n",
        "    ])\n",
        "    return train_tfms, test_tfms\n",
        "\n",
        "#  eval\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total = correct = 0\n",
        "    loss_sum = 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss_sum += F.cross_entropy(logits, y).item() * y.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    return loss_sum / total, correct / total\n",
        "\n",
        "#  train one epoch\n",
        "def train_one_epoch(model, loader, optimizer, scaler, device, criterion,\n",
        "                    scheduler, augment_mode, mixup_alpha, cutmix_alpha, use_amp):\n",
        "    model.train()\n",
        "    total = 0\n",
        "    loss_sum = 0.0\n",
        "    for x, y in tqdm(loader, leave=False, desc=\"train\"):\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "\n",
        "        if augment_mode == \"mixup\":\n",
        "            x, ya, yb, lam = mixup_data(x, y, mixup_alpha)\n",
        "            loss_fn = lambda logits: mixup_criterion(criterion, logits, ya, yb, lam)\n",
        "        elif augment_mode == \"cutmix\":\n",
        "            x, ya, yb, lam = cutmix_data(x, y, cutmix_alpha)\n",
        "            loss_fn = lambda logits: mixup_criterion(criterion, logits, ya, yb, lam)\n",
        "        else:\n",
        "            loss_fn = lambda logits: criterion(logits, y)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        # AMP (portable shim)\n",
        "        with amp_autocast(enabled=use_amp):\n",
        "            logits = model(x)\n",
        "            loss = loss_fn(logits)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        bs = y.size(0)\n",
        "        total += bs\n",
        "        loss_sum += loss.item() * bs\n",
        "    return loss_sum / total\n",
        "\n",
        "#  main\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    # overrides from args\n",
        "    overrides = {}\n",
        "    if args.save_dir is not None: overrides[\"save_dir\"] = args.save_dir\n",
        "    if args.epochs is not None: overrides[\"epochs\"] = args.epochs\n",
        "    if args.batch_size is not None: overrides[\"batch_size\"] = args.batch_size\n",
        "    if args.lr is not None: overrides.setdefault(\"optimizer\", {})[\"lr\"] = args.lr\n",
        "    if args.weight_decay is not None: overrides.setdefault(\"optimizer\", {})[\"weight_decay\"] = args.weight_decay\n",
        "    if args.mixup_alpha is not None: overrides[\"mixup_alpha\"] = args.mixup_alpha\n",
        "    if args.cutmix_alpha is not None: overrides[\"cutmix_alpha\"] = args.cutmix_alpha\n",
        "\n",
        "    cfg = get_config(args.profile, overrides)\n",
        "    set_seed(cfg[\"seed\"])\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ensure_dir(cfg[\"save_dir\"])\n",
        "    ckpt_dir = os.path.join(cfg[\"save_dir\"], \"ckpts\")\n",
        "    ensure_dir(ckpt_dir)\n",
        "    log_f, writer = csv_logger(os.path.join(cfg[\"save_dir\"], \"metrics.csv\"))\n",
        "\n",
        "    mean, std = cfg[\"data\"][\"mean\"], cfg[\"data\"][\"std\"]\n",
        "    train_tfms, test_tfms = build_transforms(mean, std)\n",
        "\n",
        "    # CIFAR-10 datasets\n",
        "    train_set = torchvision.datasets.CIFAR10(root=cfg[\"data\"][\"root\"], train=True,  download=True, transform=train_tfms)\n",
        "    test_set  = torchvision.datasets.CIFAR10(root=cfg[\"data\"][\"root\"], train=False, download=True, transform=test_tfms)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=cfg[\"batch_size\"], shuffle=True,\n",
        "                              num_workers=cfg[\"num_workers\"], pin_memory=True, drop_last=True)\n",
        "    test_loader  = DataLoader(test_set,  batch_size=cfg[\"batch_size\"], shuffle=False,\n",
        "                              num_workers=cfg[\"num_workers\"], pin_memory=True)\n",
        "\n",
        "    # model / opt / loss / AMP / scheduler\n",
        "    model = build_model(cfg[\"model\"][\"name\"], num_classes=10).to(device)\n",
        "    optimizer = build_optimizer(model, cfg)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=cfg.get(\"label_smoothing\", 0.0))\n",
        "    scaler = make_scaler(enabled=cfg[\"amp\"])  # AMP shim\n",
        "\n",
        "    steps_per_epoch = len(train_loader)\n",
        "    scheduler = build_warmup_cosine(optimizer, steps_per_epoch, cfg[\"epochs\"], cfg[\"scheduler\"][\"warmup_epochs\"])\n",
        "\n",
        "    augment_mode = cfg.get(\"augment_mode\", \"off\")\n",
        "    mixup_alpha  = cfg.get(\"mixup_alpha\", 0.0)\n",
        "    cutmix_alpha = cfg.get(\"cutmix_alpha\", 0.0)\n",
        "\n",
        "    print(f\"[cfg] profile={args.profile} | augment_mode={augment_mode} | mixup_alpha={mixup_alpha} | cutmix_alpha={cutmix_alpha} | save_dir={cfg['save_dir']}\")\n",
        "\n",
        "    best_acc = 0.0\n",
        "    try:\n",
        "        for epoch in range(cfg[\"epochs\"]):\n",
        "            tr_loss = train_one_epoch(model, train_loader, optimizer, scaler, device, criterion, scheduler,\n",
        "                                      augment_mode, mixup_alpha, cutmix_alpha, cfg[\"amp\"])\n",
        "            va_loss, va_acc = evaluate(model, test_loader, device)\n",
        "            writer.writerow({\"epoch\": epoch, \"train_loss\": tr_loss, \"val_loss\": va_loss, \"val_acc\": va_acc})\n",
        "            if va_acc > best_acc:\n",
        "                best_acc = va_acc\n",
        "                save_ckpt({\"epoch\": epoch, \"model\": model.state_dict(), \"best_acc\": best_acc},\n",
        "                          os.path.join(ckpt_dir, \"best.pth\"))\n",
        "            print(f\"epoch {epoch+1}/{cfg['epochs']} | train {tr_loss:.4f} | val {va_loss:.4f} | acc {va_acc*100:.2f}%\")\n",
        "    finally:\n",
        "        log_f.close()\n",
        "\n",
        "# entry — run MixUp, then CutMix in sequence (notebook-safe)\n",
        "if __name__ == \"__main__\":\n",
        "    # 1) MixUp\n",
        "    os.environ[\"PROFILE\"] = \"mixup\"\n",
        "    print(\"\\n==================== MIXUP RUN ====================\")\n",
        "    main()\n",
        "\n",
        "    # 2) CutMix\n",
        "    os.environ[\"PROFILE\"] = \"cutmix\"\n",
        "    print(\"\\n==================== CUTMIX RUN ====================\")\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "RbqDCkeR8nsa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "outputId": "bbaef874-44bd-4384-d187-1285246e58b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== MIXUP RUN ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 11.7MB/s]\n",
            "/tmp/ipython-input-1464226345.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.GradScaler(enabled=enabled)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[cfg] profile=mixup | augment_mode=mixup | mixup_alpha=0.4 | cutmix_alpha=0.0 | save_dir=results/mixup\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtrain:   0%|          | 0/390 [00:00<?, ?it/s]/tmp/ipython-input-1464226345.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast(enabled=enabled)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1464226345.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PROFILE\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mixup\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n==================== MIXUP RUN ====================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# 2) CutMix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1464226345.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             tr_loss = train_one_epoch(model, train_loader, optimizer, scaler, device, criterion, scheduler,\n\u001b[0m\u001b[1;32m    167\u001b[0m                                       augment_mode, mixup_alpha, cutmix_alpha, cfg[\"amp\"])\n\u001b[1;32m    168\u001b[0m             \u001b[0mva_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1464226345.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, scaler, device, criterion, scheduler, augment_mode, mixup_alpha, cutmix_alpha, use_amp)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    358\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    357\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    358\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Summarizing MixUp vs CutMix results + Softmax Visualization + CSV export\n",
        "# ==========================================================\n",
        "import os, glob, pandas as pd, torch, torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# ---- assumes build_model(...) is already defined elsewhere in your notebook ----\n",
        "# it should accept model_name=\"resnet18_cifar\" and num_classes=10 (as in training)\n",
        "\n",
        "def load_runs(root):\n",
        "    runs = []\n",
        "    for csv_path in glob.glob(os.path.join(root, \"**\", \"metrics.csv\"), recursive=True):\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "        except Exception:\n",
        "            continue\n",
        "        if len(df) == 0:\n",
        "            continue\n",
        "        runs.append({\n",
        "            \"run\": os.path.dirname(csv_path),\n",
        "            \"final_acc\": float(df[\"val_acc\"].iloc[-1])\n",
        "        })\n",
        "    return pd.DataFrame(runs)\n",
        "\n",
        "def _load_model_for_eval(model_ckpt_path, device, model_name=\"resnet18_cifar\", num_classes=10):\n",
        "    # Build the same model used in training; fallback if needed\n",
        "    try:\n",
        "        model = build_model(model_name, num_classes=num_classes).to(device)\n",
        "    except Exception:\n",
        "        # last-resort fallback: try plain torchvision resnet18 head for CIFAR10\n",
        "        from torchvision.models import resnet18\n",
        "        import torch.nn as nn\n",
        "        model = resnet18(weights=None, num_classes=num_classes)\n",
        "        # CIFAR conv1 tweak if your train-time model used 3x3 kernel/stride1\n",
        "        if getattr(model, \"conv1\", None) and model.conv1.kernel_size != (3,3):\n",
        "            model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        model = model.to(device)\n",
        "\n",
        "    state = torch.load(model_ckpt_path, map_location=device)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def _collect_max_softmax_confidences(model, testloader, device):\n",
        "    confidences = []\n",
        "    with torch.no_grad():\n",
        "        for x, _ in testloader:\n",
        "            x = x.to(device)\n",
        "            probs = F.softmax(model(x), dim=1)\n",
        "            max_conf, _ = probs.max(dim=1)\n",
        "            confidences.extend(max_conf.detach().cpu().numpy())\n",
        "    return confidences\n",
        "\n",
        "def summarize(roots, *, model_name=\"resnet18_cifar\", out_dir=\"results\"):\n",
        "    # ---- 1) Aggregate runs into tables ----\n",
        "    parts = []\n",
        "    for r in roots:\n",
        "        tag = os.path.basename(r.rstrip(\"/\"))\n",
        "        df = load_runs(r)\n",
        "        if df.empty:\n",
        "            print(f\"no runs found under {r}\")\n",
        "            continue\n",
        "        df[\"tag\"] = tag\n",
        "        parts.append(df)\n",
        "\n",
        "    if not parts:\n",
        "        print(\"no runs to summarize\")\n",
        "        return None\n",
        "\n",
        "    out = pd.concat(parts, ignore_index=True)\n",
        "\n",
        "    # Per-run table (for audit)\n",
        "    per_run = out[[\"run\", \"final_acc\", \"tag\"]].copy()\n",
        "\n",
        "    # Summary table\n",
        "    summary = out.groupby(\"tag\")[\"final_acc\"].agg([\"mean\", \"std\", \"count\"]).reset_index()\n",
        "\n",
        "    # Display (std may be NaN if count=1; that is expected)\n",
        "    display(summary.style.set_caption(\"MixUp vs CutMix Summary\").format({\"mean\": \"{:.4f}\", \"std\": \"{:.4f}\"}))\n",
        "\n",
        "    # ---- 2) Softmax Visualization (one best checkpoint per tag) ----\n",
        "    print(\"\\nGenerating Softmax confidence plots...\")\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Use the same normalization you used in training; both of these are common for CIFAR-10.\n",
        "    mean, std = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "    test_tfms = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
        "    testset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tfms)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n",
        "    plt.figure(figsize=(8,5))\n",
        "    for r in roots:\n",
        "        tag = os.path.basename(r.rstrip(\"/\"))\n",
        "        ckpt = os.path.join(r, \"ckpts\", \"best.pth\")\n",
        "        if not os.path.exists(ckpt):\n",
        "            print(f\"No checkpoint found for {tag}, skipping softmax plot.\")\n",
        "            continue\n",
        "\n",
        "        model = _load_model_for_eval(ckpt, device, model_name=model_name, num_classes=10)\n",
        "        confidences = _collect_max_softmax_confidences(model, testloader, device)\n",
        "        plt.hist(confidences, bins=30, alpha=0.5, label=f\"{tag}\")\n",
        "\n",
        "    plt.title(\"Softmax Confidence Distribution — MixUp vs CutMix\")\n",
        "    plt.xlabel(\"Predicted Probability (max softmax)\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ---- 3) CSV exports ----\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    summary_csv = os.path.join(out_dir, \"summary.csv\")\n",
        "    perrun_csv = os.path.join(out_dir, \"final_accs.csv\")\n",
        "    summary.to_csv(summary_csv, index=False)\n",
        "    per_run.to_csv(perrun_csv, index=False)\n",
        "    print(f\"\\nSaved summary to: {summary_csv}\")\n",
        "    print(f\"Saved per-run table to: {perrun_csv}\")\n",
        "\n",
        "    return per_run\n"
      ],
      "metadata": {
        "id": "yLvz1tRP89Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses the same model name you trained: \"resnet18_cifar\"\n",
        "summarize([\"results/mixup\", \"results/cutmix\"], model_name=\"resnet18_cifar\", out_dir=\"results\")\n"
      ],
      "metadata": {
        "id": "j5PBmhhIQFPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For statistical testing (see RMD)\n",
        "import pandas as pd, os\n",
        "\n",
        "mixup_df = pd.read_csv(\"results/mixup/metrics.csv\")\n",
        "cutmix_df = pd.read_csv(\"results/cutmix/metrics.csv\")\n",
        "\n",
        "mixup_df[\"tag\"] = \"mixup\"\n",
        "cutmix_df[\"tag\"] = \"cutmix\"\n",
        "\n",
        "combined = pd.concat([mixup_df, cutmix_df], ignore_index=True)\n",
        "combined.to_csv(\"results/all_metrics.csv\", index=False)\n",
        "print(\"✅ Combined CSV saved to results/all_metrics.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6Y2ZC4T-ZZv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"results/all_metrics.csv\")"
      ],
      "metadata": {
        "id": "8tMk2g2hZaak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}